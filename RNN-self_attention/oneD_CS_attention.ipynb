{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ad2505b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#注意力机制的实现\n",
    "class CoordAtt(nn.Module):\n",
    "    def __init__(self, inp, oup, reduction=32):   #输入通道数 输出通道数 缩减比例\n",
    "        super(CoordAtt, self).__init__()\n",
    "        # self.pool_w = nn.AdaptiveAvgPool1d(1)\n",
    "        self.pool_w = nn.AdaptiveMaxPool1d(1)    #自适应最大池化 保证可以有不同维度的输入\n",
    "        mip = max(6, inp // reduction)      #输入通道数衰减 但不小于6\n",
    "        self.conv1 = nn.Conv1d(inp, mip, kernel_size=1, stride=1, padding=0)\n",
    "        self.bn1 = nn.BatchNorm1d(mip, track_running_stats=False) #不追踪均值方差的平均值信息，训练时使用当前的，测试时也用当前的，适用于小样本\n",
    "        self.act = MetaAconC(mip) #自定义了一个激活函数 赋值给self.act\n",
    "        self.conv_w = nn.Conv1d(mip, oup, kernel_size=1, stride=1, padding=0)\n",
    "##总结：引入坐标注意力机制，帮助网络更好地捕获输入特征的空间关系\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = x   #保存输入 便于之后的残差连接\n",
    "        n, c, w = x.size() #获取输入张量的维度信息 n为batch大小\n",
    "        x_w = self.pool_w(x) #输入进行自适应最大池化\n",
    "        y = torch.cat([identity, x_w], dim=2) #原始输入和池化后的输入拼接 \n",
    "        y = self.conv1(y)  \n",
    "        y = self.bn1(y)\n",
    "        y = self.act(y) #通过自定义的激活函数\n",
    "        x_ww, x_c = torch.split(y, [w, 1], dim=2) #y沿宽度维度切分为2部分，包含前w个位置的信息，一个位置的信息两种\n",
    "        a_w = self.conv_w(x_ww)\n",
    "        a_w = a_w.sigmoid()\n",
    "        out = identity * a_w\n",
    "        return out\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
