{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f9a5716d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#AdamP的优化器类，加入了一些自适应权重衰减的概念\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim.optimizer import Optimizer\n",
    "import math\n",
    "\n",
    "class AdamP(Optimizer):\n",
    "    def __init__(self, params, lr=1e-3, betas=(0.9, 0.999), eps=1e-8,\n",
    "                 weight_decay=0, delta=0.1, wd_ratio=0.1, nesterov=False, amsgrad=False):\n",
    "        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay,\n",
    "                        delta=delta, wd_ratio=wd_ratio, nesterov=nesterov, amsgrad=amsgrad)\n",
    "        super(AdamP, self).__init__(params, defaults)\n",
    "\n",
    "    def _channel_view(self, x):\n",
    "        return x.view(x.size(0), -1)\n",
    "\n",
    "    def _layer_view(self, x):\n",
    "        return x.view(1, -1)\n",
    "\n",
    "    def _cosine_similarity(self, x, y, eps, view_func):\n",
    "        x = view_func(x)\n",
    "        y = view_func(y)\n",
    "\n",
    "        return F.cosine_similarity(x, y, dim=1, eps=eps).abs_()\n",
    "\n",
    "    def _projection(self, p, grad, perturb, delta, wd_ratio, eps):\n",
    "        wd = 1\n",
    "        expand_size = [-1] + [1] * (len(p.shape) - 1)\n",
    "        for view_func in [self._channel_view, self._layer_view]:\n",
    "\n",
    "            cosine_sim = self._cosine_similarity(grad, p.data, eps, view_func)\n",
    "\n",
    "            if cosine_sim.max() < delta / math.sqrt(view_func(p.data).size(1)):\n",
    "                p_n = p.data / view_func(p.data).norm(dim=1).view(expand_size).add_(eps)\n",
    "                perturb -= p_n * view_func(p_n * perturb).sum(dim=1).view(expand_size)\n",
    "                wd = wd_ratio\n",
    "\n",
    "                return perturb, wd\n",
    "\n",
    "        return perturb, wd\n",
    "    def __setstate__(self, state):\n",
    "        super(AdamP, self).__setstate__(state)\n",
    "        for group in self.param_groups:\n",
    "            group.setdefault('amsgrad', False)\n",
    "\n",
    "    def step(self, closure=None):\n",
    "        loss = None\n",
    "        if closure is not None:\n",
    "            loss = closure()\n",
    "\n",
    "        for group in self.param_groups:\n",
    "            for p in group['params']:\n",
    "                if p.grad is None:\n",
    "                    continue\n",
    "\n",
    "                grad = p.grad.data\n",
    "\n",
    "                nesterov = group['nesterov']\n",
    "                amsgrad = group['amsgrad']\n",
    "\n",
    "                state = self.state[p]\n",
    "\n",
    "                # State initialization\n",
    "                if len(state) == 0:\n",
    "                    state['step'] = 0\n",
    "                    state['exp_avg'] = torch.zeros_like(p.data)\n",
    "                    state['exp_avg_sq'] = torch.zeros_like(p.data)\n",
    "                if amsgrad:\n",
    "                    # Maintains max of all exp. moving avg. of sq. grad. values\n",
    "                    state['max_exp_avg_sq'] = torch.zeros_like(p.data)\n",
    "\n",
    "                # Adam\n",
    "                exp_avg, exp_avg_sq = state['exp_avg'], state['exp_avg_sq']\n",
    "                if amsgrad:\n",
    "                    max_exp_avg_sq = state['max_exp_avg_sq']\n",
    "                beta1, beta2 = group['betas']\n",
    "\n",
    "\n",
    "                state['step'] += 1\n",
    "\n",
    "\n",
    "\n",
    "                exp_avg.mul_(beta1).add_(grad, alpha=1 - beta1)\n",
    "                exp_avg_sq.mul_(beta2).addcmul_(grad, grad, value=1 - beta2)\n",
    "                if amsgrad:\n",
    "                    # Maintains the maximum of all 2nd moment running avg. till now\n",
    "                    torch.max(max_exp_avg_sq, exp_avg_sq, out=max_exp_avg_sq)\n",
    "                    # Use the max. for normalizing running avg. of gradient\n",
    "                    # denom = max_exp_avg_sq.sqrt().add_(group['eps'])\n",
    "                    denom = max_exp_avg_sq.sqrt().add_(group['eps'])\n",
    "                else:\n",
    "                    denom = exp_avg_sq.sqrt().add_(group['eps'])\n",
    "\n",
    "                bias_correction1 = 1 - beta1 ** state['step']\n",
    "                bias_correction2 = 1 - beta2 ** state['step']\n",
    "\n",
    "                step_size = group['lr'] * math.sqrt(bias_correction2) / bias_correction1\n",
    "\n",
    "                if nesterov:\n",
    "                    perturb = (beta1 * exp_avg + (1 - beta1) * grad) / denom\n",
    "                else:\n",
    "                    perturb = exp_avg / denom\n",
    "\n",
    "                # Projection\n",
    "                wd_ratio = 1\n",
    "                if len(p.shape) > 1:\n",
    "                    perturb, wd_ratio = self._projection(p, grad, perturb, group['delta'], group['wd_ratio'], group['eps'])\n",
    "\n",
    "                # Weight decay\n",
    "                if group['weight_decay'] > 0:\n",
    "                    p.data.mul_(1 - group['lr'] * group['weight_decay'] * wd_ratio)\n",
    "\n",
    "                # Step\n",
    "                p.data.add_(perturb, alpha=-step_size)\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb3ebb2b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
