{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "448b8301",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2912, 384)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy import io as sio\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import random_split\n",
    "from torch.utils.data import TensorDataset, DataLoader, random_split\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# 读取数据\n",
    "vib_data_fault1 =  sio.loadmat('D:\\Desktop\\shujuronghe_20230316/vib_fault1.mat')['vib_fault1'].astype(np.float32)\n",
    "curr_data_fault1 = sio.loadmat('D:\\Desktop\\shujuronghe_20230316/curr_fault1.mat')['curr_fault1'].astype(np.float32)\n",
    "vib_data_fault2 = sio.loadmat('D:\\Desktop\\shujuronghe_20230316/vib_fault2.mat')['vib_fault2'].astype(np.float32)\n",
    "curr_data_fault2 = sio.loadmat('D:\\Desktop\\shujuronghe_20230316/curr_fault2.mat')['curr_fault2'].astype(np.float32)\n",
    "vib_data_fault3 =sio.loadmat('D:\\Desktop\\shujuronghe_20230316/vib_fault3.mat')['vib_fault3'].astype(np.float32)\n",
    "curr_data_fault3 =sio.loadmat('D:\\Desktop\\shujuronghe_20230316/curr_fault3.mat')['curr_fault3'].astype(np.float32)\n",
    "vib_data_fault4 =sio.loadmat('D:\\Desktop\\shujuronghe_20230316/vib_fault4.mat')['vib_fault4'].astype(np.float32)\n",
    "curr_data_fault4 =sio.loadmat('D:\\Desktop\\shujuronghe_20230316/curr_fault4.mat')['curr_fault4'].astype(np.float32)\n",
    "vib_data_fault5 =  sio.loadmat('D:\\Desktop\\shujuronghe_20230316/vib_fault5.mat')['vib_fault5'].astype(np.float32)\n",
    "curr_data_fault5 = sio.loadmat('D:\\Desktop\\shujuronghe_20230316/curr_fault5.mat')['curr_fault5'].astype(np.float32)\n",
    "vib_data_fault6 = sio.loadmat('D:\\Desktop\\shujuronghe_20230316/vib_fault7.mat')['vib_fault7'].astype(np.float32)\n",
    "curr_data_fault6 = sio.loadmat('D:\\Desktop\\shujuronghe_20230316/curr_fault7.mat')['curr_fault7'].astype(np.float32)\n",
    "\n",
    "\n",
    "\n",
    "print(vib_data_fault4.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "62796524",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([384, 2, 2912])\n"
     ]
    }
   ],
   "source": [
    "#转置成（384，2912）\n",
    "vib_data_fault1 =vib_data_fault1.T \n",
    "vib_data_fault2 =vib_data_fault2.T\n",
    "vib_data_fault3 =vib_data_fault3.T\n",
    "vib_data_fault4 =vib_data_fault4.T\n",
    "vib_data_fault5 =vib_data_fault5.T \n",
    "vib_data_fault6 =vib_data_fault6.T\n",
    "\n",
    "\n",
    "curr_data_fault1=curr_data_fault1.T\n",
    "curr_data_fault2=curr_data_fault2.T\n",
    "curr_data_fault3=curr_data_fault3.T\n",
    "curr_data_fault4=curr_data_fault4.T\n",
    "curr_data_fault5=curr_data_fault5.T\n",
    "curr_data_fault6=curr_data_fault6.T\n",
    "\n",
    "# 将二维数组扩展为形状为(384, 1, 2912),再合并数组为（384，2，2912）\n",
    "vib_data_fault1= np.expand_dims(vib_data_fault1, axis=1)\n",
    "vib_data_fault2= np.expand_dims(vib_data_fault2, axis=1)\n",
    "vib_data_fault3= np.expand_dims(vib_data_fault3, axis=1)\n",
    "vib_data_fault4= np.expand_dims(vib_data_fault4, axis=1)\n",
    "vib_data_fault5= np.expand_dims(vib_data_fault5, axis=1)\n",
    "vib_data_fault6= np.expand_dims(vib_data_fault6, axis=1)\n",
    "\n",
    "curr_data_fault1=np.expand_dims(curr_data_fault1, axis=1)\n",
    "curr_data_fault2=np.expand_dims(curr_data_fault2, axis=1)\n",
    "curr_data_fault3=np.expand_dims(curr_data_fault3, axis=1)\n",
    "curr_data_fault4=np.expand_dims(curr_data_fault4, axis=1)\n",
    "curr_data_fault5=np.expand_dims(curr_data_fault5, axis=1)\n",
    "curr_data_fault6=np.expand_dims(curr_data_fault6, axis=1)\n",
    "\n",
    "\n",
    "#合并振动和电流\n",
    "data_fault1 = np.concatenate((vib_data_fault1, curr_data_fault1), axis=1)\n",
    "data_fault2 = np.concatenate((vib_data_fault2, curr_data_fault2), axis=1)\n",
    "data_fault3 = np.concatenate((vib_data_fault3, curr_data_fault3), axis=1)\n",
    "data_fault4 = np.concatenate((vib_data_fault4, curr_data_fault4), axis=1)\n",
    "data_fault5 = np.concatenate((vib_data_fault5, curr_data_fault5), axis=1)\n",
    "data_fault6 = np.concatenate((vib_data_fault6, curr_data_fault6), axis=1)\n",
    "# 将数据转换为 PyTorch 张量\n",
    "data_fault1 = torch.tensor(data_fault1, dtype=torch.float32)\n",
    "data_fault2 = torch.tensor(data_fault2, dtype=torch.float32)\n",
    "data_fault3 = torch.tensor(data_fault3, dtype=torch.float32)\n",
    "data_fault4 = torch.tensor(data_fault4, dtype=torch.float32)\n",
    "data_fault5 = torch.tensor(data_fault5, dtype=torch.float32)\n",
    "data_fault6 = torch.tensor(data_fault6, dtype=torch.float32)\n",
    "\n",
    "print(data_fault4.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b1d4a57",
   "metadata": {},
   "source": [
    "一些说明：\n",
    "数据集的划分通常只在训练开始前进行一次\n",
    "每个epoch的minibatch都会会被重新打乱\n",
    "训练集需要被打乱，测试集不需要"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "f4027a08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training samples:  1612\n",
      "Number of testing samples:  692\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 合并数据和标签为一个TensorDataset对象\n",
    "    #torch.cat张量拼接,默认按照第一个维度拼接，如果是第二个维度可以dim=1\n",
    "    #torch.tensor创建张量\n",
    "dataset = TensorDataset(torch.cat((data_fault1, data_fault2, data_fault3,data_fault4,data_fault5, data_fault6)), \n",
    "                        torch.tensor([0]*384 + [1]*384 + [2]*384 +[3]*384 +[4]*384 +[5]*384))\n",
    "\n",
    "# 划分训练集和测试集\n",
    "    #random_split是生成随机数再进行划分 随机的\n",
    "train_size = int(len(dataset) * 0.7)\n",
    "test_size = len(dataset) - train_size\n",
    "train_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n",
    "\n",
    "# 创建训练集和测试集的DataLoader对象\n",
    "    #batch是一次使用小批量数据 计算平均梯度来更新参数\n",
    "    #每一个epoch都会打乱数据\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=90, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=90, shuffle=False)\n",
    "\n",
    "# 打印训练集和测试集的样本数\n",
    "print(\"Number of training samples: \", len(train_dataset))\n",
    "print(\"Number of testing samples: \", len(test_dataset))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c37f426",
   "metadata": {},
   "source": [
    "以下定义了我的卷积神经网络，其中super是继承了nn.module类。\n",
    "对于卷积层的说明：有几个卷积核就有几个输出通道，1个卷积核一次是对于2个输入通道同时进行卷积运算提取特征，也就是说会一次处理两层特征曾。每一个卷积核都在初始化阶段 随机初始化权重，因而每个卷积核开始时学习的“起点”不同，虽然拥有相同的结构，但依然会提取到不同的特征。\n",
    "对于池化层的说明，这里使用了一维池化层，所谓一维是对每个通道独立进行，每次池化后数据长度减半。\n",
    "对于全连接层的说明："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "01ae24bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FusionNet(nn.Module):\n",
    "    def __init__(self, hidden_size):\n",
    "        super(FusionNet, self).__init__() #super是为了继承自nn.module类，调用了父类的初始化方法\n",
    "        self.conv1 = nn.Conv1d(2, 32, kernel_size=3, padding=1)  # 第一层卷积层（输入通道数，输出通道数，卷积核大小，左右各补1个0）\n",
    "        self.relu1 = nn.ReLU()  # 激活函数\n",
    "        self.pool1 = nn.MaxPool1d(kernel_size=2, stride=2)  # 最大池化层（池化窗口大小，滑动步幅）\n",
    "        self.conv2 = nn.Conv1d(32, 64, kernel_size=3, padding=1)  # 第二层卷积层\n",
    "        self.relu2 = nn.ReLU()  # 激活函数\n",
    "        self.pool2 = nn.MaxPool1d(kernel_size=2, stride=2)  # 最大池化层\n",
    "        self.conv3 = nn.Conv1d(64, 128, kernel_size=3, padding=1)  # 第三层卷积层\n",
    "        self.relu3 = nn.ReLU()  # 激活函数\n",
    "        self.pool3 = nn.MaxPool1d(kernel_size=2, stride=2)  # 最大池化层\n",
    "        self.conv4 = nn.Conv1d(128, 256, kernel_size=3, padding=1)  # 第四层卷积层\n",
    "        self.relu4 = nn.ReLU()  # 激活函数\n",
    "        self.pool4 = nn.MaxPool1d(kernel_size=2, stride=2)  # 最大池化层\n",
    "        self.fc1 = nn.Linear(256 * 182, hidden_size)  # 全连接层（通道数*池化层输出的长度，输出层的维度）\n",
    "        self.dropout = nn.Dropout(0.5)  # Dropout层\n",
    "        \n",
    "    def forward(self, x): #这里要注意连接的顺序，init里定义时则不需要\n",
    "        x = self.pool1(self.relu1(self.conv1(x)))  # 第一层卷积层的前向传播\n",
    "        x = self.pool2(self.relu2(self.conv2(x)))  # 第二层卷积层的前向传播\n",
    "        x = self.pool3(self.relu3(self.conv3(x)))  # 第三层卷积层的前向传播\n",
    "        x = self.pool4(self.relu4(self.conv4(x)))  # 第四层卷积层的前向传播\n",
    "        x = x.view(x.shape[0], -1)  # 将卷积层的输出展平为一维张量\n",
    "        x = self.dropout(x)  # Dropout层的前向传播\n",
    "        x = self.fc1(x)  # 全连接层的前向传播\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d297e0b",
   "metadata": {},
   "source": [
    "没有实例化，没有给input_size等赋值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "99de2785",
   "metadata": {},
   "outputs": [],
   "source": [
    "#分类网络\n",
    "class Net2(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_classes):\n",
    "        super(Net2, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size) #全连接层\n",
    "        self.relu = nn.ReLU()  #激活层\n",
    "        self.fc2 = nn.Linear(hidden_size, num_classes) #全连接层\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.softmax(x)\n",
    "        return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "e5262d3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_size = 1000\n",
    "fusion_net = FusionNet(hidden_size=1000)\n",
    "#input_size是通过查找fusion_net的输出特征数（fusion_net.fc1.out_features）获得的\n",
    "net2 = Net2(input_size=fusion_net.fc1.out_features, hidden_size=500, num_classes=6)\n",
    "model = nn.Sequential(fusion_net, net2)\n",
    "#模型的输入是fusion_net的输入，即三通道的信号数据。经过fusion_net的卷积、池化和全连接层之后，得到一个特征向量，。然后，这个特征向量会被传递到net2，net2会将其映射到一个长度为num_classes的向量，表示三个类别的概率分布\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "6ba96d28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义损失函数和优化器\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95e3ae36",
   "metadata": {},
   "source": [
    "这个函数定义没有self，是一个独立的训练函数但不是类的方法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "c3644148",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, dataloader, criterion, optimizer):\n",
    "    model.train()     #将模型调整到训练模式\n",
    "    running_loss = 0.0    #每个mini-batch的平均损失\n",
    "    running_corrects = 0  #当前迭代过程中预测正确的样本数\n",
    "    total_samples = 0     #当前迭代过程中总的样本数\n",
    "    for i, data in enumerate(dataloader, 0): #遍历dataloader中的每个minibatch 起始索引 0\n",
    "        inputs, labels = data           #讲每个mini-batch分解为两个变量\n",
    "        optimizer.zero_grad()      #清除梯度缓存\n",
    "        outputs = model(inputs)   #模型的输出\n",
    "        loss = criterion(outputs, labels)     #损失值 criterion计算\n",
    "        loss.backward()    #计算loss的梯度\n",
    "        optimizer.step()   #更新模型参数\n",
    "        running_loss += loss.item() #累加每个mini-batch的loss，为了之后计算整个epoch的loss\n",
    "        _, preds = torch.max(outputs, 1) #模型输出的每个样本在各类别上的分数，然后将分数最高的类别作为该样本的预测结果\n",
    "        running_corrects += torch.sum(preds == labels.data) #当前迭代过程中预测正确的样本数。\n",
    "        total_samples += labels.size(0) #当前迭代过程中的总样本数。\n",
    "    acc = running_corrects.double() / total_samples\n",
    "    return running_loss / len(dataloader), acc  #每个mini-batch的平均损失和预测准确率\n",
    "\n",
    "def test(model, dataloader):\n",
    "    model.eval() #切换到评估模式\n",
    "    correct = 0 #初始化正确的预测数\n",
    "    total = 0  #初始化总样本\n",
    "    with torch.no_grad(): #关闭pytorch的梯度追踪\n",
    "        for data in dataloader:\n",
    "            inputs, labels = data  #输入数据和标签\n",
    "            outputs = model(inputs)  #模型的输出\n",
    "            _, predicted = torch.max(outputs.data, 1) #取概率最高的作为预测结果，1表示在维度1上取最大值\n",
    "            #返回1最大值的张量，2对应索引的张量，_表示我们不关心第一个只需要第二个，也就是样本预测类别索引\n",
    "            total += labels.size(0) #累加当前batch中的样本数到总正确预测数中\n",
    "            #(predicted == labels)是布尔张量，结果为True/False，求和，取python标量值\n",
    "            correct += (predicted == labels).sum().item() #累加当前batch中正确的预测样本数 到总正确预测数中\n",
    "    return correct / total\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f62f87f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Train Loss: 1.792, Train Accuracy: 0.167, Test Accuracy: 0.143\n",
      "Epoch: 2, Train Loss: 1.781, Train Accuracy: 0.231, Test Accuracy: 0.324\n",
      "Epoch: 3, Train Loss: 1.703, Train Accuracy: 0.311, Test Accuracy: 0.324\n",
      "Epoch: 4, Train Loss: 1.656, Train Accuracy: 0.335, Test Accuracy: 0.342\n",
      "Epoch: 5, Train Loss: 1.651, Train Accuracy: 0.334, Test Accuracy: 0.488\n",
      "Epoch: 6, Train Loss: 1.642, Train Accuracy: 0.364, Test Accuracy: 0.324\n",
      "Epoch: 7, Train Loss: 1.642, Train Accuracy: 0.346, Test Accuracy: 0.421\n",
      "Epoch: 8, Train Loss: 1.642, Train Accuracy: 0.367, Test Accuracy: 0.361\n",
      "Epoch: 9, Train Loss: 1.639, Train Accuracy: 0.362, Test Accuracy: 0.361\n",
      "Epoch: 10, Train Loss: 1.637, Train Accuracy: 0.364, Test Accuracy: 0.490\n",
      "Epoch: 11, Train Loss: 1.629, Train Accuracy: 0.385, Test Accuracy: 0.408\n",
      "Epoch: 12, Train Loss: 1.630, Train Accuracy: 0.380, Test Accuracy: 0.366\n",
      "Epoch: 13, Train Loss: 1.632, Train Accuracy: 0.373, Test Accuracy: 0.324\n",
      "Epoch: 14, Train Loss: 1.637, Train Accuracy: 0.355, Test Accuracy: 0.389\n",
      "Epoch: 15, Train Loss: 1.623, Train Accuracy: 0.388, Test Accuracy: 0.413\n",
      "Epoch: 16, Train Loss: 1.625, Train Accuracy: 0.384, Test Accuracy: 0.366\n",
      "Epoch: 17, Train Loss: 1.614, Train Accuracy: 0.398, Test Accuracy: 0.457\n",
      "Epoch: 18, Train Loss: 1.624, Train Accuracy: 0.378, Test Accuracy: 0.325\n",
      "Epoch: 19, Train Loss: 1.618, Train Accuracy: 0.382, Test Accuracy: 0.487\n",
      "Epoch: 20, Train Loss: 1.603, Train Accuracy: 0.408, Test Accuracy: 0.497\n",
      "Epoch: 21, Train Loss: 1.595, Train Accuracy: 0.427, Test Accuracy: 0.467\n",
      "Epoch: 22, Train Loss: 1.577, Train Accuracy: 0.447, Test Accuracy: 0.527\n",
      "Epoch: 23, Train Loss: 1.581, Train Accuracy: 0.439, Test Accuracy: 0.475\n",
      "Epoch: 24, Train Loss: 1.572, Train Accuracy: 0.453, Test Accuracy: 0.582\n",
      "Epoch: 25, Train Loss: 1.555, Train Accuracy: 0.466, Test Accuracy: 0.543\n",
      "Epoch: 26, Train Loss: 1.539, Train Accuracy: 0.488, Test Accuracy: 0.543\n",
      "Epoch: 27, Train Loss: 1.521, Train Accuracy: 0.528, Test Accuracy: 0.568\n",
      "Epoch: 28, Train Loss: 1.496, Train Accuracy: 0.551, Test Accuracy: 0.660\n",
      "Epoch: 29, Train Loss: 1.471, Train Accuracy: 0.587, Test Accuracy: 0.736\n",
      "Epoch: 30, Train Loss: 1.449, Train Accuracy: 0.627, Test Accuracy: 0.686\n",
      "Epoch: 31, Train Loss: 1.413, Train Accuracy: 0.661, Test Accuracy: 0.763\n",
      "Epoch: 32, Train Loss: 1.404, Train Accuracy: 0.643, Test Accuracy: 0.750\n",
      "Epoch: 33, Train Loss: 1.370, Train Accuracy: 0.702, Test Accuracy: 0.759\n",
      "Epoch: 34, Train Loss: 1.343, Train Accuracy: 0.738, Test Accuracy: 0.861\n",
      "Epoch: 35, Train Loss: 1.317, Train Accuracy: 0.767, Test Accuracy: 0.714\n",
      "Epoch: 36, Train Loss: 1.313, Train Accuracy: 0.741, Test Accuracy: 0.821\n",
      "Epoch: 37, Train Loss: 1.267, Train Accuracy: 0.820, Test Accuracy: 0.961\n",
      "Epoch: 38, Train Loss: 1.250, Train Accuracy: 0.849, Test Accuracy: 0.965\n",
      "Epoch: 39, Train Loss: 1.235, Train Accuracy: 0.856, Test Accuracy: 0.975\n",
      "Epoch: 40, Train Loss: 1.221, Train Accuracy: 0.859, Test Accuracy: 0.896\n",
      "Epoch: 41, Train Loss: 1.209, Train Accuracy: 0.879, Test Accuracy: 0.874\n",
      "Epoch: 42, Train Loss: 1.189, Train Accuracy: 0.894, Test Accuracy: 0.954\n"
     ]
    }
   ],
   "source": [
    "# 训练模型\n",
    "num_epochs =170\n",
    "train_loss = []\n",
    "train_acc = []\n",
    "test_acc = []\n",
    "for epoch in range(num_epochs):\n",
    "    loss, acc = train(model, train_dataloader, criterion, optimizer)\n",
    "    train_loss.append(loss)\n",
    "    train_acc.append(acc)\n",
    "    test_acc.append(test(model, test_dataloader))\n",
    "    print('Epoch: %d, Train Loss: %.3f, Train Accuracy: %.3f, Test Accuracy: %.3f' %\n",
    "          (epoch+1, loss, acc, test_acc[-1]))\n",
    "    # 每个epoch结束后绘制图像\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(train_loss)\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Train Loss')\n",
    "plt.title('Training Loss')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(test_acc, label='Test Accuracy')\n",
    "plt.plot(train_acc, label='Train Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Train and Test Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3a6a24a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, dataloader):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for data in dataloader:\n",
    "            inputs, labels = data\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    accuracy = correct / total\n",
    "    return accuracy\n",
    "\n",
    "test_acc = evaluate(model, test_dataloader)\n",
    "print('Test Accuracy: %.3f' % test_acc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ec8905fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lenovo\\数据融合\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.path.abspath('.'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32a1c68d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
